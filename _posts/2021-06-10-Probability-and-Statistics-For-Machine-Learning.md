 
# **Table Of Content -** 
* Introduction
* Population and Sample
* Gaussian Distribution
* Skewness
* Kurtosis
* Standard Normal Variate
* Kernel Density Estimation
* Central Limit Theorem
* Quantile Quantile Plots
* Bernoulli Distribution
* Chebyshew Inequality
* Log Normal Distribution
* Power Law Distribution


# **Introduction -** 

Probability is all about handling the uncertainty. Uncertainty involves making decisions based on incomplete information and this is how the whole world operates. Probability is the field of mathematics that gives tools to quantify the uncertanty of the events and reason in principled manner. Machine learning is all about building a predictive model by using uncertain data. There are three main reasons which causes uncertainty in the machine learning which are noisy data, incomplete coverage of problem domain, imperfect models. These all uncertainties can be managed by using tools of probabilty. Some of the aspects in Machine Learning where probability is used are - 

* Classification models must predict a probability of class membership.
* Algorithms are designed using probability (e.g. Naive Bayes).
* Learning algorithms will make decisions using probability (e.g. information gain).
* Sub-fields of study are built on probability (e.g. Bayesian networks).
* Algorithms are trained under probability frameworks (e.g. maximum likelihood).
* Models are fit using probabilistic loss functions (e.g. log loss and cross entropy).
* Model hyperparameters are configured with probability (e.g. Bayesian optimization).
* Probabilistic measures are used to evaluate model skill (e.g. brier score, ROC).

This is the reason why a profound knowledge of probability can be a game changer to understand the machine learning models in depth. So there are lots of thing to cover in this blog so lets dive in with some simple concepts - 

# Population And Sample-




# **References -**
* https://machinelearningmastery.com/probability-for-machine-learning/ 




